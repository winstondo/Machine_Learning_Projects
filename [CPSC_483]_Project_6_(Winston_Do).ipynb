{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[CPSC 483] Project 6 (Winston Do).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BX5BK0kgnen"
      },
      "source": [
        "#CPSC 483 - Project 6\n",
        "##Winston Do\n",
        "This project was focused on training classification models using Gaussian Naive Bayes GNB, k Nearest Neighbors (KNN) and support vector machine (SVM). The unique attribute of this dataset is that it contains a lot of categorical datapoints. This entailed a degree of data engineering to clean the data before it could be used to train the models.\n",
        "\n",
        "###Data cleaning\n",
        "The data contained numerous features that were categorical in nature so a function was written to clean the data, as well as remove the 'duration' feature as outlined by the probject prompt.\n",
        "```\n",
        "#cleans dataframes as per the project requirements, maybe could be generalized for other applications.\n",
        "def CleanDataFrame(dataframe, target_name, removed_features=None, use_dummy_coding=False, use_zScore_Normalize=False):\n",
        "  featureNamesList = list(dataframe.columns)\n",
        "  featureNamesList.remove(target_name)\n",
        "  if (removed_features != None): #if feature names are provided, removes them from the input dataframe otherwise skips\n",
        "    for feature in removed_features:\n",
        "      featureNamesList.remove(feature)\n",
        "  X = dataframe[featureNamesList]\n",
        "  t = dataframe[[target_name]]\n",
        "  ```\n",
        "The above function will seperate the dataframe into the feature matrix and target vector as well as apply one-hot encoding to the categorical variables. \n",
        "\n",
        "Initially, it was found that the SVM model had the highest fit accuracy with KNN coming close. However, some data resampling and more metrics were needed to decalre a specific classifier model as \"best\".\n",
        "\n",
        "Confusion matrixes (CM) and an area under the curve (AUC) score was calculated. These metrics showed the relative poor performance of the classifiers without resampling.\n",
        "\n",
        "The data was then resampled with the fit_resample() function. \n",
        "\n",
        "These yielded a better accuracy result and the AUC and CMs were calculated using the helper function:\n",
        "```\n",
        "def GetAUCandConfusionMatrix(model, feature, target):\n",
        "  prediction = model.predict(feature)\n",
        "  AUC_score = roc_auc_score(target, prediction)\n",
        "  confuse_mat = confusion_matrix(target, prediction)\n",
        "  return AUC_score, confuse_mat\n",
        "```\n",
        "The resampled data was then used to train new models which were held in a dicitonary:\n",
        "```\n",
        "ModelDict_resam = {'GNB': GNBModel_resampled, 'KNN': KNNModel_resampled, 'SVM': SVMModel_resampled}\n",
        "```\n",
        "\n",
        "With the resampled data, KNN seems to have the best AUC score. The confusion matrix yeilds the lowest amount of false negatives. But when balanced against the false positive rate. The SVM performs much better both the false negatives and positive amounts are low. \n",
        "\n",
        "The GNB classifier seems to yield an alarming amount of false positives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtzeT9B4I0ok"
      },
      "source": [
        "#CPSC 483\n",
        "#Project 6\n",
        "#Winston Do\n",
        "\n",
        "############################\n",
        "#Modules and helper function block\n",
        "########################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as sk\n",
        "from google.colab import files\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def printShape(data):\n",
        "  try:\n",
        "    print(data.shape)\n",
        "  except: \n",
        "    print('Error: printShape must be panda dataframe obj as argument')\n",
        "  \n",
        "def printType(obj):\n",
        "  print(type(obj))\n",
        "\n",
        "def uploadManyFiles(NumberOfFiles):\n",
        "  for x in range(NumberOfFiles):\n",
        "    x = files.upload()\n",
        "\n",
        "#uses pd Dataframe: \n",
        "#input: takes in a list of column names , string with the target column name, dataframe\n",
        "#outputs two pddataframes\n",
        "def sliceDataFrame(feature_list, target, pdDataFrame):\n",
        "  return pdDataFrame[feature_list], pdDataFrame[[target]]\n",
        "\n",
        "#performs zscore normilization on a column vector of the dataframe. also ignores categorical columns\n",
        "def zScoreNormalize(dataframe):\n",
        "    # copy the dataframe\n",
        "    dataframe_std = dataframe.copy()\n",
        "    # apply the z-score method\n",
        "    for column in dataframe.columns:\n",
        "      if (dataframe[column].dtypes != 'object'): #ignore categorial columns of object type\n",
        "        dataframe_std[column] = (dataframe_std[column] - dataframe_std[column].mean()) / dataframe_std[column].std()        \n",
        "    return dataframe_std\n",
        "\n",
        "\n",
        "\n",
        "#cleans dataframes as per the project requirements, maybe could be generalized for other applications.\n",
        "#dependancies: pd Dataframe, zScoreNormalize(df)\n",
        "#input: PDDataframe, string of target column, [optional]: features to be removed from target vector\n",
        "#input(params): \n",
        "#outputs two pddataframes, the input and target\n",
        "def CleanDataFrame(dataframe, target_name, removed_features=None, use_dummy_coding=False, use_zScore_Normalize=False):\n",
        "  featureNamesList = list(dataframe.columns)\n",
        "  featureNamesList.remove(target_name)\n",
        "  if (removed_features != None): #if feature names are provided, removes them from the input dataframe otherwise skips\n",
        "    for feature in removed_features:\n",
        "      featureNamesList.remove(feature)\n",
        "  X = dataframe[featureNamesList]\n",
        "  t = dataframe[[target_name]]\n",
        "\n",
        "  #X, t = sliceDataFrame(featureNamesList, target_name, dataframe)\n",
        "  if (use_zScore_Normalize == True):  #standardization if argment is set to true. Must be before dummy coding\n",
        "    X = zScoreNormalize(X)\n",
        "  if (use_dummy_coding == True):\n",
        "    X = pd.get_dummies(X, drop_first=False)\n",
        "    t = pd.get_dummies(t, drop_first=True)\n",
        "  return X, t\n",
        "\n"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KXL0AhHHxZP"
      },
      "source": [
        "###########################\n",
        "#import and clean data block\n",
        "###########################\n",
        "\n",
        "#features to remove\n",
        "removedFeatures = ['duration']\n",
        "try:\n",
        "  pdDF_trainRaw = pd.read_csv('bank-additional.csv', sep=';')\n",
        "  pdDF_testRaw = pd.read_csv('bank-additional-full.csv', sep=';')\n",
        "except FileNotFoundError:\n",
        "  x = input(\"File not found. Enter how many files to input and set directory:\")\n",
        "  uploadManyFiles(int(x))\n",
        "  pdDF_trainRaw = pd.read_csv('bank-additional.csv', sep=';')\n",
        "  pdDF_testRaw = pd.read_csv('bank-additional-full.csv', sep=';')\n",
        "\n",
        "\n",
        "featureMatrix_train, targetVector_train = CleanDataFrame(pdDF_trainRaw, 'y', removed_features=removedFeatures, use_dummy_coding=True, use_zScore_Normalize=True)\n",
        "featureMatrix_test, targetVector_test = CleanDataFrame(pdDF_testRaw, 'y', removed_features=removedFeatures, use_dummy_coding=True, use_zScore_Normalize=True)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxCQIOejHYdx",
        "outputId": "3b5cf294-5cc6-46b0-e9b4-72c23a4ce315"
      },
      "source": [
        "#train a gaussian naive bayes classifier \n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "GNBModel = GaussianNB().fit(featureMatrix_train, np.ravel(targetVector_train))\n",
        "GNBModel_score = GNBModel.score(featureMatrix_test, targetVector_test)\n",
        "print(\"Score of Gaussian Naive Bayes is\", GNBModel_score)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score of Gaussian Naive Bayes is 0.5958288821986987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz54MvP0HY8b",
        "outputId": "b0935dae-91e5-4867-a5d2-6033dc093aaa"
      },
      "source": [
        "#train a k nearest neighbors(k=5) model\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "K = 5\n",
        "KNNModel = KNeighborsClassifier(n_neighbors=K).fit(featureMatrix_train, np.ravel(targetVector_train))\n",
        "KNNModel_score = KNNModel.score(featureMatrix_test, targetVector_test)\n",
        "print(f\"Score of k({K}) Nearest Neighbors is {KNNModel_score}\")"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score of k(5) Nearest Neighbors is 0.893682626007575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzEZDzYFHgwB",
        "outputId": "b1fdb2b3-733f-4efc-8e7c-a32ce70153c9"
      },
      "source": [
        "#train a SVM model\n",
        "from sklearn import svm\n",
        "SVMModel = svm.SVC()\n",
        "SVMModel.fit(featureMatrix_train, np.ravel(targetVector_train))\n",
        "SVMModel_score = SVMModel.score(featureMatrix_test, targetVector_test)\n",
        "print(\"Score of Suppor Vector Machine is\", SVMModel_score)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score of Suppor Vector Machine is 0.8993153345634651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0dpVHcUL_J3"
      },
      "source": [
        "The classifier with the highest accuracy seems to be the support vector machine (SVM) at 0.899 but with a slightly higher margin than the k nearest nieghbors (KNN) at 0.892."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xqb3hcwM3VS",
        "outputId": "ca3318da-3501-4b3f-8dfc-8ebdb2ab5973"
      },
      "source": [
        "ModelDict = {'GNB': GNBModel, 'KNN': KNNModel, 'SVM': SVMModel}\n",
        "targetVector_train.value_counts(normalize=True)\n",
        "\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "y_yes\n",
              "0        0.890507\n",
              "1        0.109493\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTw4g005QaN2"
      },
      "source": [
        "Only ~11% of the customers have subscribed to the product according to the training data.\n",
        "\n",
        "If the frequency of subscriptions is 0, then the models would not be very accurate it describing individuals who would subscribe to the product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmB5NtCTQh2L",
        "outputId": "8c521a04-4f84-49c8-a08f-b833440d7615"
      },
      "source": [
        "#create confusion matrix\n",
        "targetVector_train_zeros = np.zeros_like(targetVector_train)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "dumb_confusion_matrix = confusion_matrix(targetVector_train, targetVector_train_zeros,)\n",
        "dumb_confusion_matrix\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3668,    0],\n",
              "       [ 451,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2X6BeQqV2Pc",
        "outputId": "e048fcdc-c5e3-45fb-bf72-9b16aa52c994"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "\n",
        "def GetAUCandConfusionMatrix(model, feature, target):\n",
        "  prediction = model.predict(feature)\n",
        "  AUC_score = roc_auc_score(target, prediction)\n",
        "  confuse_mat = confusion_matrix(target, prediction)\n",
        "  return AUC_score, confuse_mat\n",
        "\n",
        "\n",
        "\n",
        "for model in ModelDict:\n",
        "  AUC, c_mat = GetAUCandConfusionMatrix(ModelDict[model], featureMatrix_train, targetVector_train)\n",
        "  print(f\"AUCScore of {model} is: {AUC}\")\n",
        "  print(f\"Confustion matrix of {model} is:\\n\", c_mat)\n",
        "\n",
        "\n"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUCScore of GNB is: 0.6744768683187972\n",
            "Confustion matrix of GNB is:\n",
            " [[2077 1591]\n",
            " [  98  353]]\n",
            "AUCScore of KNN is: 0.6491127797914243\n",
            "Confustion matrix of KNN is:\n",
            " [[3607   61]\n",
            " [ 309  142]]\n",
            "AUCScore of SVM is: 0.6123185602332875\n",
            "Confustion matrix of SVM is:\n",
            " [[3638   30]\n",
            " [ 346  105]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW9csS49Z91E"
      },
      "source": [
        "Based on the AUC scores for all models, it seems like the best classifier is the gaussian naive bayes model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x_RWsbSaEeJ",
        "outputId": "81ced500-0449-428f-827a-5599304b61e0"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler \n",
        "\n",
        "ros = RandomOverSampler(random_state=(2021-4-22))\n",
        "X_res_train, t_res_train = ros.fit_resample(featureMatrix_train, np.ravel(targetVector_train))\n",
        "\n",
        "\n",
        "GNBModel_resampled = GaussianNB().fit(X_res_train, np.ravel(t_res_train))\n",
        "GNBModel_resampled_score = GNBModel_resampled.score(featureMatrix_test, targetVector_test)\n",
        "print(\"Score of Gaussian Naive Bayes is\", GNBModel_resampled_score)\n",
        "\n",
        "\n",
        "#train a k nearest neighbors(k=5) model\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "K = 5\n",
        "KNNModel_resampled = KNeighborsClassifier(n_neighbors=K).fit(X_res_train, np.ravel(t_res_train))\n",
        "KNNModel_resampled_score = KNNModel_resampled.score(featureMatrix_test, targetVector_test)\n",
        "print(f\"Score of k({K}) Nearest Neighbors is {KNNModel_resampled_score}\")\n",
        "#train a SVM model\n",
        "from sklearn import svm\n",
        "SVMModel_resampled = svm.SVC()\n",
        "SVMModel_resampled.fit(X_res_train, np.ravel(t_res_train))\n",
        "SVMModel_resampled_score = SVMModel_resampled.score(featureMatrix_test, targetVector_test)\n",
        "print(\"Score of Suppor Vector Machine is\", SVMModel_resampled_score)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Score of Gaussian Naive Bayes is 0.5628338350976012\n",
            "Score of k(5) Nearest Neighbors is 0.7643488394678062\n",
            "Score of Suppor Vector Machine is 0.8514130329222104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOLtqaffflkB",
        "outputId": "3ce30d66-a9bb-428c-a808-86ca004c7a46"
      },
      "source": [
        "ModelDict_resam = {'GNB': GNBModel_resampled, 'KNN': KNNModel_resampled, 'SVM': SVMModel_resampled}\n",
        "\n",
        "for model in ModelDict_resam:\n",
        "  AUC, c_mat = GetAUCandConfusionMatrix(ModelDict_resam[model], featureMatrix_train, targetVector_train)\n",
        "  print(f\"AUCScore of {model} is: {AUC}\")\n",
        "  print(f\"Confustion matrix of {model} is:\\n\", c_mat)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUCScore of GNB is: 0.6575464193226248\n",
            "Confustion matrix of GNB is:\n",
            " [[1904 1764]\n",
            " [  92  359]]\n",
            "AUCScore of KNN is: 0.9169484630059942\n",
            "Confustion matrix of KNN is:\n",
            " [[3075  593]\n",
            " [   2  449]]\n",
            "AUCScore of SVM is: 0.7993148631297953\n",
            "Confustion matrix of SVM is:\n",
            " [[3310  358]\n",
            " [ 137  314]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvi3KA0af80y"
      },
      "source": [
        "The KNN seems to have the best AUC score when resampling the data. The confusion matrix yeilds the lowest amount of false negatives. But when balanced against the false positive rate. The SVM performs much better both the false negatives and positive amounts are low. \n",
        "\n",
        "The GNB classifier seems to yield an alarming amount of false positives."
      ]
    }
  ]
}